VLLM‌（Vector Large Language Model）是一种大型语言模型，通常用于推理加速和高效部署。
VLLM的性能飞跃主要体现在从推理加速到全方位优化的过程。
VLLM可以通过不同的推理方法来实现，包括贪婪搜索、Beam Search和随机采样等方式。
这些方法通过调整超参数（如do_sample、temperature和top_k）来控制模型的生成方式和多样性‌