* 机器学习
  * 定义：机器学习是人工智能的一个子领域，专注于开发算法和模型，使计算机能够从数据中学习并做出决策或预测，而无需显式编程
  * 特点：
    * 数据驱动：依赖大量数据进行学习。
    * 自动化：通过算法自动调整模型参数，以优化性能。
    * 多种算法：包括监督学习、无监督学习、半监督学习和强化学习等

* 深度学习
  * 定义： 深度学习是机器学习的一个子领域，专注于使用深层神经网络来解决复杂的问题。深度学习模型通常包含多个隐藏层，能够学习数据的高层次抽象特征。
  * 特点：
    * 多层结构：深度学习模型通常包含多个隐藏层，层数可以达到几十甚至上百层。
    * 自动特征学习：通过多层结构，模型能够自动学习数据的高层次特征，而不需要手动设计特征。
    * 强大的表达能力：深度学习模型能够捕捉数据中的复杂模式，适用于处理高维数据。
 
* 神经网络
  * 定义： 神经网络是一种模拟人脑神经元结构的计算模型，由多个神经元（节点）组成，这些神经元通过连接（边）相互连接。神经网络可以分为多个层次，包括输入层、隐藏层和输出层
  * 特点：
    * 多层结构：神经网络可以有多层，每一层包含多个神经元。
    * 非线性变换：通过激活函数引入非线性，使模型能够学习复杂的模式。
    * 参数学习：通过反向传播算法和优化器调整模型参数，以最小化损失函数
   
* 大模型
  * 定义： 大模型是指参数量非常大的神经网络模型，通常包含数亿甚至数十亿个参数。这些模型通常在大规模数据集上进行训练，以捕捉更丰富的特征和模式。
  * 特点：
    * 超大规模：参数量巨大，通常在几亿到几十亿之间。
    * 高性能：在许多任务上表现出卓越的性能，尤其是在少样本学习和迁移学习方面。
    * 计算资源需求高：训练和推理需要大量的计算资源，通常使用高性能GPU或TPU集群。
    * 预训练和微调：通常采用预训练和微调的方法，先在大规模数据集上进行预训练，再在特定任务上进行微调。

* 注意力机制
  * 注意力机制（Attention Mechanism）是深度学习中的一种技术，旨在帮助模型在处理序列数据时更加关注重要的部分，从而提高模型的性能。注意力机制最初在自然语言处理（NLP）任务中得到广泛应用，但后来也被应用于计算机视觉和其他领域
    * 自注意力（Self-Attention）：查询、键和值来自同一个序列，用于捕捉序列内部的依赖关系。
    * 交叉注意力（Cross-Attention）：查询来自一个序列，键和值来自另一个序列，用于捕捉两个序列之间的依赖关系。
    * 多头注意力（Multi-Head Attention）：将自注意力机制应用多次，每次使用不同的线性投影，最后将结果拼接起来，增强模型的表达能力。

* pyTorch 
  * 是一个通用的深度学习框架，提供了灵活的张量计算和自动求导功能。

* Transformer
  * 是一个专门用于 NLP 任务的库，提供了多种预训练的 Transformer 模型和工具。
  * 基于自注意力机制的编码器-解码器架构。
  
* 预训练
  * 在大规模文本数据集上进行预训练，学习通用的语言表示。
  
* 微调
  * 在特定任务上进行微调，以适应具体任务的需求。


