### 常见的激活函数
激活函数在神经网络中起着至关重要的作用，它们为模型引入非线性，使得神经网络能够学习和表示复杂的函数
* ReLU (Rectified Linear Unit)：
  * 公式：( f(x) = \max(0, x) )
  * 特点：输出范围在 [0, +∞) 之间，
    * 简单高效，计算速度快；
    * 能够有效缓解梯度消失问题；
    * 但可能会导致“死区”问题（即某些神经元的输出始终为零）。
  * 适用场景：广泛应用于卷积神经网络和深度神经网络。

* Sigmoid：
  * 公式：( f(x) = \frac{1}{1 + e^{-x}} )
  * 特点：将输入压缩到 (0, 1) 之间，适用于二分类问题；但容易导致梯度消失。
  * 适用场景：逻辑回归、二分类问题。

* Tanh (Hyperbolic Tangent)：
  * 公式：( f(x) = \tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}} )
  * 特点：将输入压缩到 (-1, 1) 之间，输出均值接近零；但同样容易导致梯度消失。
  * 适用场景：隐藏层、RNN。

* Leaky ReLU：
  * 公式：( f(x) = \max(\alpha x, x) )，其中 (\alpha) 是一个小的正数（如 0.01）
  * 特点：解决了 ReLU 的“死区”问题，允许负值通过；但选择合适的 (\alpha) 可能需要调参。
  * 适用场景：深度神经网络。

* Softmax：
  * 公式：( f(x_i) = \frac{e^{x_i}}{\sum_{j=1}^{n} e^{x_j}} )
  * 特点：将输入向量转换为概率分布，适用于多分类问题。
  * 适用场景：输出层、多分类问题。
  
* Swish：
  * 公式：( f(x) = x \cdot \sigma(x) )，其中 (\sigma(x)) 是 Sigmoid 函数
  * 特点：非单调函数，性能优于 ReLU；但计算复杂度较高。
  * 适用场景：深度神经网络

### 选择激活函数的考虑因素
* 问题类型：分类问题、回归问题等。
* 网络结构：输入层、隐藏层、输出层等。
* 训练效果：梯度消失、梯度爆炸等问题。