### 梯度的升高和降低主要与以下几个因素相关：
* 学习率：学习率决定了参数更新的步长。如果学习率设置得过高，可能会导致梯度在参数空间中跳跃过大，从而使得损失函数值不降反升；
        反之，若学习率过低，则可能导致训练过程过于缓慢。
* 损失函数的选择：不同的损失函数对模型参数的敏感度不同，这会影响梯度的大小。例如，均方误差损失函数在预测值与真实值差距较大时会产生较大的梯度。
* 激活函数：激活函数的选择也会影响梯度的传播。
          某些激活函数（如Sigmoid或Tanh）在输入值较大或较小时会导致梯度接近于零，即所谓的“梯度消失”问题；
          而ReLU及其变体则可以缓解这一问题，因为它们在正区间内具有恒定的梯度。
* 网络结构：深层网络更容易遇到梯度消失或梯度爆炸的问题。随着网络深度的增加，前向传播中的微小变化可能会被放大，导致后向传播时梯度变得非常大或非常小。
* 数据规范化：输入数据的尺度对梯度也有影响。通过标准化或归一化处理输入数据，可以使梯度更加稳定，有助于加速训练过程。
* 权重初始化：适当的权重初始化方法可以帮助避免梯度消失或梯度爆炸的问题。
          例如，Xavier初始化或He初始化等策略能够根据网络层的输入输出节点数来调整初始权重的分布，以促进更好的梯度流动。

* 梯度消失：在深度神经网络中，如果所有层都是线性的，梯度在反向传播过程中可能会逐渐变小，导致训练困难。激活函数可以帮助缓解这一问题。
* 梯度爆炸：某些激活函数（如ReLU）具有恒定的梯度，有助于防止梯度爆炸